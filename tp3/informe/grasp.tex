\section{Metaheurística de Grasp}

\subsection{Algoritmo}

\indent Nuestra implementación de Grasp opera de la siguiente manera: Se generan una cantidad de veces determinada por parámetro de soluciones con nuestra implementación de la heurística golosa. Luego se elige una de ellas pseudoaleatoriamente y se le aplica nuestra implementación de búsqueda local. Si esa solución obtenida con búsqueda locar mejora la que teníamos anteriormente, nos quedamos con ella. Este proceso se iterará una cantidad de veces máxima determinada por parámetro. Sin embargo, puede ocurrir que se deje de iterar antes, si no se encontraron mejores en una cantidad determinada de iteración, también provista por parámetro. A continuación detallamos más detenidamente nuestra implementación.\\

\indent El comportamiento de nuestra implementación de Grasp se ve fuertemente influenciado por ciertos parámetros. A saber, estos son: $porcentaje$, que se usará para obtener las soluciones con nuestras implementaciones del algoritmo goloso y de búsqueda local; $maxRCL$, que determinará la cantidad de candidatos obtenidos mediante la aplicación del algoritmo goloso; $maxIteraciones$ que determinará la cantidad de veces máxima que iterará el algoritmo en el peor caso; y finalmente $maxIterSinMejora$ que determinará la máxima cantidad de iteraciones que permitiremos ejecutarse sin que se obtenga una mejor solución antes de dejar de iterar (donde una mejor solución es aquella que mejora el impacto del coloreo de G en H). \\
\indent Como se puede observar, los últimos dos parámetros descriptos se utilizarán como criterios de parada: a lo sumo el algoritmo iterará $maxIteraciones$ de veces en busca de soluciones, a menos que en una cantidad de iteraciones consecutivas igual a $maxIteracionesSinMejora$ no se obtenga una solución que implique un mayor impacto del coloreo de G con esa solución en H.\\

\indent El algoritmo iterará, en el peor caso, hasta la máxima cantidad de iteraciones permitidas. En cada iteración, se calculan $maxRCL$ (RCL proviene de Restrictive Candidate List) soluciones con el algoritmo goloso que implementamos, cada una de ellas usando el valor de $porcentaje$, es decir que se calculan $maxRCL$ candidatos golosos a los cuales se les puede aplicar el algoritmo de búsqueda local que diseñamos. De esos candidatos se elige uno pseudoaleatoriamente (en nuestro caso, al implementarlo en C++, hicimos uso de la función rand()). A ese candidato elegido, se le aplica nuestra implementación de búsqueda local, obteniendo así una nueva solución. Si esta solución mejora el impacto de G en H en comparación con la solución que se manejaba hasta el momento, se reemplaza a esa solución vieja por esta nueva y se resetea un contador que contiene la cantidad de veces que se iteró sin lograr una mejora. Caso contrario se aumenta dicho contador y se chequea que no se haya alcanzado la máxima cantidad de iteraciones sin mejoras permitidas, en cuyo caso se dejará de iterar y se devolverá la mejor solución que se obtuvo hasta el momento.\\
\indent Este procedimiento se ejecutará hasta que se cumpla con algunos de los criterios de parada. Cuando uno de ellos se cumpla se devuelve la mejor solución que se obtuvo hasta el momento. Es de notar que en cada iteración se generan $maxRCL$ candidatos golosos nuevos de los cuáles se elige uno para continuar con la búsqueda local.\\
\indent La aleatorización de los candidatos golosos se obtiene mediante una combinación de los parámetros $porcentaje$ y $maxRCL$. El primer parámetro impacta en la solución que devuelve el algoritmo goloso que implementamos como describimos en la sección correspondiente. El segundo parámetro determina la cantidad de soluciones candidatas que se calcularán en un principio. Luego, además, de esos $maxRCL$ candidatos se elegirá uno pseudoaleatoriamente (como mencionamos antes, en nuestra implementación usamos la función rand() de C++).\\

\begin{algorithm}[H]
\caption{} 
\begin{codebox}
\Procname{$\proc{maximoImpactoGrasp}(Grafo$ g$, Grafo$ h$, double $ porcentaje$, unsigned$ $int$ maxIteraciones$, unsigned$ $int$ maxIterSinMejora,$ unsigned$ $int$ maxRCL$)$}

\li vector$<$unsigned int$>$ res(n + 1)
\li res[0] $\gets$ 0
\li unsigned int sinMejora $\gets$ 0
\li
\li vector$<$unsigned int$>$ coloreo(n,1) // Todos los elementos valen 1
\li
\li \For i desde 0 hasta maxIteraciones \Do 
\li vector$<$vector$<$unsigned int$>>$ rcl(maxRCL)
\li 	\For k desde 0 hasta maxRCL \Do
\li 		rcl[k] $\gets$ $maximoImpactoGoloso(g,h, porcentaje)$
		\End
\li
\li		unsigned int e $\gets$ índice de uno de los elementos de rcl elegido al azar
\li
\li 	vector$<$unsigned int$>$ solBusqLocal $\gets$ maximoImpactoLocal(g,h,porcentaje,rcl[e])
\li
\li 	\If solBusqLocal[0]$>$res[0] \Do
\li			res[0] =solBusqLocal[0]
\li
\li			\For k desde 1 hasta n \Do
\li				res[k]=solBusqLocal[k]
\li
			\End
\li			sinMejora$\gets$ 0
		%\End
\li		\Else \Do
\li			sinMejora++;
\li         \If sinMejora == maxIterSinMejora \Do
            
\li                salir del ciclo
            
            \End
        \End

	\End	
\li
\li return res
\End
\end{codebox}
\end{algorithm}

\subsection{Análisis de complejidad}

\indent Analicemos la complejidad de maximoImpactoGrasp. Los primeros pasos del algoritmos son crear unos vectores igual a la cantidad de nodos de los grafos. Eso cuesta O(n) para cada creación de vector.\\
\indent Luego se itera maxIteraciones veces. El costo de cada iteración es el siguiente:\\
\indent Primero se calcular maxRCL veces soluciones con maximoImpactoGoloso, donde maxRCL la cantidad de restrictive candidates list, es decir la cantidad máxima de candidatos golosos a utilizar.Ese ciclo cuesta entonces O(maxRCL*(n*(n+m)+ $n^{3}$) de acuerdo a nuestro análisis de complejidad de maximoImpactoGoloso.\\
\indent A continuación, se elige pseudoaleatoriamente uno de esos candidatos.\\
\indent Una vez elegido un candidato, se aplica maximoImpactoLocal con dicha solución golosa.\\ Por lo que analizamos en la sección correspondiente, esto cuesta O(n*(n+m)+ $n^{3}$ +$ n^{2}$*(n+m)).\\
\indent Una vez hecho esto, se decide si se va a quedar con la nueva solución obtenida con maximoImpactoLocal y esto cuesta O(n).\\
\indent Luego, el ciclo cuesta :\\

O(maxIteraciones* [(maxRCL*(n*(n+m)+ $n^{3})$)+ (n*(n+m)+ $n^{3}$ +$ n^{2}$*(n+m))] )\\

 que además es la complejidad de maximoImpactoGrasp.\\


\subsection{Experimentación y Resultados}



\subsubsection{Optimización de parámetros}

\quad Primero buscamos optimizar los parámetros que utiliza nuestra implementación. Los cuales son el parámetro de la heurística golosa aleatoria que usa como base para ir armando las lista de candidatos, y los criterios de parada de una cantidad máxima de iteraciones y una cantidad máxima de iteraciones sin mejora en la solución que se va obteniendo.

\quad Para testear, medir y generar resultados usamos el concepto de distancia en este caso como la diferencia entre el valor máximo que se obtiene del valor de impacto para determinado grafo y el valor obtenido con la heuristica con ciertos valores de parámetros determinados.

\begin{itemize}

\item \textbf{Parámetro de la Heurística Constructiva Golosoa Aleatoria}

\quad

\quad Usamos como base la herística golosa aleatoria que desarrollamos anteriormente. Variamos el parámetro que recibe desde 0.05 hasta 1 con saltos de 0.05. Recordemos que cuando más chico es el valor, la heuristica golosa elige aleatoriamente entre menos candidatos posibles para ir coloreando, siendo si es valor 1, elige entre cualquier nodo posible.

\quad Testeamos con 50, 100 y 150 nodos y 100 repeticiones para cada cantidad de los mismos. Los grafos elegidos para generar resultados son grafos generados al azar porque generar diversos tipos de grafos.

\quad

\quad Los resultados obtenidos con 50 nodos:

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.6]{optimizacionGRASPParGoloso1.png}
\caption{Costos}
\end{figure}

\quad

\quad Con 100 nodos no se pudo apreciar diferencias con respecto a lo anterior.

\quad

\quad Los resultados obtenidos con 150 nodos:

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.6]{optimizacionGRASPParGoloso2.png}
\caption{Costos}
\end{figure}

\quad Tras obtener estos resultados, decidimos que cuando el grafo posea menos de 50 nodos el valor sea 0.3, en cambio, cuando tenga más nodos sea de 0.1. Esto significa que cuando más \textit{chico} es el grafo, en el goloso aleatorio se elige al azar entre menor cantidad de nodos para colorear de cierta forma. Osea que cuando se construye nuestra lista de candidatos posibles, cuanto más \textit{grande} es el grafo, se necesita menor porcentaje de aleatoriedad para crear posibles soluciones distintas entre si.

\quad

\item \textbf{Parámetro Cantidad Máxima de Iteraciones}

\quad

\quad Como en nuestra implementación, este criterio de parada depende de la cantidad de nodos del grafo, decidimos variar para optimizarlo con los valores de 0.5 hasta 5, con pasos de 0.5. Es decir, es un coeficiente que multiplica la cantidad de nodos: siendo 2 se itera 2 veces la cantidad de nodos, y cuando es 5 se itera 5 veces la cantidad de nodos. Elegimos ese rango porque tenemos en cuanta que cuantas más iteraciones, más es el costo computacional de la heuristica.

\quad Al igual que antes testeamos con 50, 100 y 150 nodos y 100 repeticiones para cada cantidad de los mismos. Los grafos elegidos para generar resultados son grafos generados al azar porque generar diversos tipos de grafos.

\quad Lo que obtuvimos en promedio:

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.6]{optimizacionGRASPParIterMax.png}
\caption{Costos}
\end{figure}

\quad Se puede observar que a partir del valor 3 del parámetro, lo que se mejora de la soluciones que se van obteniendo es menos. Creemos que es el punto ideal entre costo computacional y mejora de las soluciones parciales. Sin embargo, cuando son grafos pequeños, la cantidad de iteraciones es poca por lo que probamos con menor cantidad de nodos y un rango para el parámetro mayor. Obtuvimos que lo óptimo sería el valor 5.

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.6]{optimizacionGRASPParIterMax2.png}
\caption{Costos}
\end{figure}

\quad Aumentar el valor del parámetro de 3 a 5 no implicaria un gran costo computacional debido a que es menor la cantidad de nodos. Entonces queda que para grafos de menos de 50 nodos se utiliza la herística con cantidad máxima de iteraciones 5 veces la cantidad de nodos. Con grafos \textit{más grandes} se utiliza 3 veces la cantidad de nodos.

\item \textbf{Parámetro Cantidad de Iteraciones Máximas Sin Mejora}

\quad

\quad Con este criterio de parada, determinamos si pasa cierta cantidad de iteraciones sin que se haya mejorado la solucón parcial obtenida, se finaliza. Este parámetro determina el porcentaje de las iteraciones máximas que tienen que pasar sin que se mejore para terminar. Por eso, los rangos posibles que definimos son de 0.1 a 0.9.

\quad Al igual que con los otros dos parámetros, probamos con grafos aleatorios de 50,100 y 150 nodos con 100 repeticiones para cada cantidad.

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.6]{optimizacionGRASPParIterMaxSinMejora.png}
\caption{Costos}
\end{figure}

\quad Podemos observar como a partir del valor 0.5, se estabiliza la mejora por lo que consideramos a ese valor como el óptimo. Es decir, que si no se obtiene una mejora en la solución parcial obtenida en esa cantidad de iteraciones se termina, osea, la mitad de la cantidad máxima de iteraciones de la heurística. Ejemplo: si son 500 iteraciones máximas, si se cumple que en 250 iteraciones consecutivas no hay mejora, se finaliza.

\end{itemize}

\subsubsection{Costo temporal}

\quad Vamos a comparar el costo obtenido por nuestra heuristica versus el costo calculado teóricamente en la sección de complejidad.

\quad Vamos a testear con 3 tipos de grafos: al azar, grafos estrellas no uniformes, grafos redes de 4 vértices.